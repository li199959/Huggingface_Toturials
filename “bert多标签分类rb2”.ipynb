{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/li199959/one/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB1%E2%80%9D.ipynb",
      "authorship_tag": "ABX9TyM7pBUqWWwYCvG8mE+4+719",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/Huggingface_Toturials/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BBrb2%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EhpxN3Gwdhgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88419c2-6317-4f02-83c4-6a9d405c3e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json, time \n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "bert_path = \"/content/drive/MyDrive/rb2/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
      ],
      "metadata": {
        "id": "ky5y_ZFBd55Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_masks, input_types,  = [], [], []  # input char ids, segment type ids,  attention mask\n",
        "labels = []         # 标签\n",
        "maxlen = 20      # 取30即可覆盖99%\n",
        " \n",
        "with open(\"news_title_dataset.csv\", encoding='utf-8') as f:\n",
        "    for i, line in tqdm(enumerate(f)): \n",
        "        title, y = line.strip().split('\\t')\n",
        "\n",
        "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
        "        # 根据参数会短则补齐，长则切断\n",
        "        encode_dict = tokenizer.encode_plus(text=title, max_length=maxlen,padding='max_length', truncation=True)\n",
        "        \n",
        "        input_ids.append(encode_dict['input_ids'])\n",
        "        input_types.append(encode_dict['token_type_ids'])\n",
        "        input_masks.append(encode_dict['attention_mask'])\n",
        "\n",
        "        labels.append(int(y))\n",
        "\n",
        "input_ids, input_types, input_masks = np.array(input_ids), np.array(input_types), np.array(input_masks)\n",
        "labels = np.array(labels)\n",
        "print(input_ids.shape, input_types.shape, input_masks.shape, labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYodgs1eLYC",
        "outputId": "2d75be9d-823a-4c37-981b-b96bc518b7f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [00:00, 1108.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 20) (600, 20) (600, 20) (600,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机打乱索引\n",
        "idxes = np.arange(input_ids.shape[0])\n",
        "np.random.seed(2019)   # 固定种子\n",
        "np.random.shuffle(idxes)\n",
        "print(idxes.shape, idxes[:10])\n",
        "\n",
        "\n",
        "# 8:1:1 划分训练集、验证集、测试集\n",
        "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:480]], input_ids[idxes[480:540]], input_ids[idxes[540:]]\n",
        "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:480]], input_masks[idxes[480:540]], input_masks[idxes[540:]] \n",
        "input_types_train, input_types_valid, input_types_test = input_types[idxes[:480]], input_types[idxes[480:540]], input_types[idxes[540:]]\n",
        "\n",
        "y_train, y_valid, y_test = labels[idxes[:480]], labels[idxes[480:540]], labels[idxes[540:]]\n",
        "\n",
        "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
        "      input_ids_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mijdSBjzH4",
        "outputId": "4d906d6a-435f-43cd-e84c-51f96ff8b0c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600,) [520 266 127 527 260 351 515 391 478  41]\n",
            "(480, 20) (480,) (60, 20) (60,) (60, 20) (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2  # 如果会出现OOM问题，减小它\n",
        "# 训练集\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
        "                           torch.LongTensor(input_masks_train), \n",
        "                           torch.LongTensor(input_types_train), \n",
        "                           torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)  \n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 验证集\n",
        "valid_data = TensorDataset(torch.LongTensor(input_ids_valid), \n",
        "                          torch.LongTensor(input_masks_valid),\n",
        "                          torch.LongTensor(input_types_valid), \n",
        "                          torch.LongTensor(y_valid))\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 测试集（是没有标签的）\n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
        "                          torch.LongTensor(input_masks_test),\n",
        "                          torch.LongTensor(input_types_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_WgcrPVtk_qh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义model\n",
        "class Bert_Model(nn.Module):\n",
        "    def __init__(self, bert_path, classes=3):\n",
        "        super(Bert_Model, self).__init__()\n",
        "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
        "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
        "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
        "        logit = self.fc(out_pool)   #  [bs, classes]\n",
        "        return logit"
      ],
      "metadata": {
        "id": "0MsQTgjllJ6Y"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameter_number(model):\n",
        "    #  打印模型参数量\n",
        "    total_num = sum(p.numel() for p in model.parameters())\n",
        "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "model = Bert_Model(bert_path).to(DEVICE)\n",
        "print(get_parameter_number(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REZf4QmWlXsD",
        "outputId": "76384737-f7dc-4181-b0c0-f84398a9e27c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/rb2/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 61004803, Trainable parameters: 61004803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(train_loader))"
      ],
      "metadata": {
        "id": "DqR29ttklpaL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估模型性能，在验证集上\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_true, val_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
        "    \n",
        "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
        "\n",
        "\n",
        "# 测试集没有标签，需要预测提交\n",
        "def predict(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "    return val_pred\n",
        "\n",
        "\n",
        "def train_and_eval(model, train_loader, valid_loader, \n",
        "                   optimizer, scheduler, device, epoch):\n",
        "    best_acc = 0.0\n",
        "    patience = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for i in range(epoch):\n",
        "        \"\"\"训练模型\"\"\"\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
        "        train_loss_sum = 0.0\n",
        "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
        "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
        "            y_pred = model(ids, att, tpe)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()   # 学习率变化\n",
        "            \n",
        "            train_loss_sum += loss.item()\n",
        "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
        "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
        "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
        "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
        "\n",
        "        \"\"\"验证模型\"\"\"\n",
        "        model.eval()\n",
        "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
        "        ## 保存最优模型\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
        "        \n",
        "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
        "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
      ],
      "metadata": {
        "id": "kRPhSVzUmT2Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPOvI8wmX3U",
        "outputId": "f2c6e547-7882-48d6-83a6-b7b574ad0533"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training epoch 1 *****\n",
            "Epoch 0001 | Step 0048/0240 | Loss 1.1869 | Time 1.4916\n",
            "Epoch 0001 | Step 0096/0240 | Loss 1.1222 | Time 2.9518\n",
            "Epoch 0001 | Step 0144/0240 | Loss 1.0908 | Time 4.4104\n",
            "Epoch 0001 | Step 0192/0240 | Loss 1.0827 | Time 5.8738\n",
            "Epoch 0001 | Step 0240/0240 | Loss 1.0247 | Time 7.4152\n",
            "current acc is 0.6000, best acc is 0.6000\n",
            "time costed = 8.28269s \n",
            "\n",
            "***** Running training epoch 2 *****\n",
            "Epoch 0002 | Step 0048/0240 | Loss 0.7987 | Time 1.6811\n",
            "Epoch 0002 | Step 0096/0240 | Loss 0.7792 | Time 3.3935\n",
            "Epoch 0002 | Step 0144/0240 | Loss 0.7400 | Time 4.8949\n",
            "Epoch 0002 | Step 0192/0240 | Loss 0.7006 | Time 6.3527\n",
            "Epoch 0002 | Step 0240/0240 | Loss 0.6988 | Time 7.8114\n",
            "current acc is 0.7333, best acc is 0.7333\n",
            "time costed = 8.44327s \n",
            "\n",
            "***** Running training epoch 3 *****\n",
            "Epoch 0003 | Step 0048/0240 | Loss 0.4346 | Time 1.4905\n",
            "Epoch 0003 | Step 0096/0240 | Loss 0.4114 | Time 2.9597\n",
            "Epoch 0003 | Step 0144/0240 | Loss 0.3695 | Time 4.4253\n",
            "Epoch 0003 | Step 0192/0240 | Loss 0.3608 | Time 5.9720\n",
            "Epoch 0003 | Step 0240/0240 | Loss 0.3594 | Time 7.6212\n",
            "current acc is 0.8167, best acc is 0.8167\n",
            "time costed = 8.6004s \n",
            "\n",
            "***** Running training epoch 4 *****\n",
            "Epoch 0004 | Step 0048/0240 | Loss 0.1812 | Time 1.7293\n",
            "Epoch 0004 | Step 0096/0240 | Loss 0.2058 | Time 3.2064\n",
            "Epoch 0004 | Step 0144/0240 | Loss 0.1800 | Time 4.6820\n",
            "Epoch 0004 | Step 0192/0240 | Loss 0.2041 | Time 6.1420\n",
            "Epoch 0004 | Step 0240/0240 | Loss 0.1881 | Time 7.5955\n",
            "current acc is 0.8667, best acc is 0.8667\n",
            "time costed = 8.24205s \n",
            "\n",
            "***** Running training epoch 5 *****\n",
            "Epoch 0005 | Step 0048/0240 | Loss 0.0889 | Time 1.4744\n",
            "Epoch 0005 | Step 0096/0240 | Loss 0.0779 | Time 2.9354\n",
            "Epoch 0005 | Step 0144/0240 | Loss 0.0754 | Time 4.4969\n",
            "Epoch 0005 | Step 0192/0240 | Loss 0.0668 | Time 6.1324\n",
            "Epoch 0005 | Step 0240/0240 | Loss 0.0755 | Time 7.8130\n",
            "current acc is 0.9333, best acc is 0.9333\n",
            "time costed = 8.84082s \n",
            "\n",
            "***** Running training epoch 6 *****\n",
            "Epoch 0006 | Step 0048/0240 | Loss 0.0154 | Time 1.4896\n",
            "Epoch 0006 | Step 0096/0240 | Loss 0.0262 | Time 2.9469\n",
            "Epoch 0006 | Step 0144/0240 | Loss 0.0265 | Time 4.4108\n",
            "Epoch 0006 | Step 0192/0240 | Loss 0.0243 | Time 5.8741\n",
            "Epoch 0006 | Step 0240/0240 | Loss 0.0354 | Time 7.3400\n",
            "current acc is 0.8833, best acc is 0.9333\n",
            "time costed = 7.46856s \n",
            "\n",
            "***** Running training epoch 7 *****\n",
            "Epoch 0007 | Step 0048/0240 | Loss 0.0297 | Time 1.4745\n",
            "Epoch 0007 | Step 0096/0240 | Loss 0.0248 | Time 2.9922\n",
            "Epoch 0007 | Step 0144/0240 | Loss 0.0222 | Time 4.6210\n",
            "Epoch 0007 | Step 0192/0240 | Loss 0.0198 | Time 6.2551\n",
            "Epoch 0007 | Step 0240/0240 | Loss 0.0191 | Time 7.9098\n",
            "current acc is 0.9333, best acc is 0.9333\n",
            "time costed = 8.03729s \n",
            "\n",
            "***** Running training epoch 8 *****\n",
            "Epoch 0008 | Step 0048/0240 | Loss 0.0096 | Time 1.4554\n",
            "Epoch 0008 | Step 0096/0240 | Loss 0.0100 | Time 2.9211\n",
            "Epoch 0008 | Step 0144/0240 | Loss 0.0089 | Time 4.4136\n",
            "Epoch 0008 | Step 0192/0240 | Loss 0.0081 | Time 6.3151\n",
            "Epoch 0008 | Step 0240/0240 | Loss 0.0084 | Time 7.8083\n",
            "current acc is 0.9333, best acc is 0.9333\n",
            "time costed = 8.02019s \n",
            "\n",
            "***** Running training epoch 9 *****\n",
            "Epoch 0009 | Step 0048/0240 | Loss 0.0079 | Time 2.7147\n",
            "Epoch 0009 | Step 0096/0240 | Loss 0.0077 | Time 5.6211\n",
            "Epoch 0009 | Step 0144/0240 | Loss 0.0076 | Time 7.3386\n",
            "Epoch 0009 | Step 0192/0240 | Loss 0.0074 | Time 8.8066\n",
            "Epoch 0009 | Step 0240/0240 | Loss 0.0072 | Time 10.2788\n",
            "current acc is 0.9167, best acc is 0.9333\n",
            "time costed = 10.40421s \n",
            "\n",
            "***** Running training epoch 10 *****\n",
            "Epoch 0010 | Step 0048/0240 | Loss 0.0043 | Time 1.4639\n",
            "Epoch 0010 | Step 0096/0240 | Loss 0.0067 | Time 2.9487\n",
            "Epoch 0010 | Step 0144/0240 | Loss 0.0073 | Time 4.4168\n",
            "Epoch 0010 | Step 0192/0240 | Loss 0.0071 | Time 5.8985\n",
            "Epoch 0010 | Step 0240/0240 | Loss 0.0069 | Time 7.4285\n",
            "current acc is 0.9167, best acc is 0.9333\n",
            "time costed = 7.60475s \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载最优权重对测试集测试\n",
        "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
        "pred_test = predict(model, test_loader, DEVICE)\n",
        "# print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(\"\\n Test Accuracy = {:.3f} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(classification_report(y_test, pred_test, digits=4))"
      ],
      "metadata": {
        "id": "7h6u65AkmiDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec96d1a-798b-4978-ddb3-28b368d996f6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30it [00:00, 218.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy = 0.867 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9032    0.9333    0.9180        30\n",
            "           1     0.8500    0.8095    0.8293        21\n",
            "           2     0.7778    0.7778    0.7778         9\n",
            "\n",
            "    accuracy                         0.8667        60\n",
            "   macro avg     0.8437    0.8402    0.8417        60\n",
            "weighted avg     0.8658    0.8667    0.8659        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = 'BBU与AUU告警'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "d2b-G_38KTBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a566405a-ad70-4b31-ee5a-29e203a68ff6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bb', '##u', '与', 'au', '##u', '告', '警']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_added_toks = tokenizer.add_tokens([\"BBU\",\"AUU\"])\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQdXg6HUgTDY",
        "outputId": "7dcb5102-f8f6-46a6-d215-90d191bb9bbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bbu', '与', 'auu', '告', '警']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tokenizer(text)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrpdXltXqK24",
        "outputId": "fd591277-6ca0-4c14-8386-1dd70edd8ed0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 21128, 680, 21129, 1440, 6356, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovN5idn0qfvI"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}