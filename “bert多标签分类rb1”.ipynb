{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/li199959/one/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB1%E2%80%9D.ipynb",
      "authorship_tag": "ABX9TyPMSW4xOctILZ3Ug18+J1Qq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/Huggingface_Toturials/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BBrb1%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhpxN3Gwdhgx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json, time \n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "bert_path = \"/content/drive/MyDrive/rb1/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
      ],
      "metadata": {
        "id": "ky5y_ZFBd55Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_masks, input_types,  = [], [], []  # input char ids, segment type ids,  attention mask\n",
        "labels = []         # 标签\n",
        "maxlen = 20      # 取30即可覆盖99%\n",
        " \n",
        "with open(\"news_title_dataset.csv\", encoding='utf-8') as f:\n",
        "    for i, line in tqdm(enumerate(f)): \n",
        "        title, y = line.strip().split('\\t')\n",
        "\n",
        "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
        "        # 根据参数会短则补齐，长则切断\n",
        "        encode_dict = tokenizer.encode_plus(text=title, max_length=maxlen,padding='max_length', truncation=True)\n",
        "        \n",
        "        input_ids.append(encode_dict['input_ids'])\n",
        "        input_types.append(encode_dict['token_type_ids'])\n",
        "        input_masks.append(encode_dict['attention_mask'])\n",
        "\n",
        "        labels.append(int(y))\n",
        "\n",
        "input_ids, input_types, input_masks = np.array(input_ids), np.array(input_types), np.array(input_masks)\n",
        "labels = np.array(labels)\n",
        "print(input_ids.shape, input_types.shape, input_masks.shape, labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYodgs1eLYC",
        "outputId": "92f937cd-97a6-4dd0-8bd7-264d889d5f86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [00:00, 1665.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 20) (600, 20) (600, 20) (600,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机打乱索引\n",
        "idxes = np.arange(input_ids.shape[0])\n",
        "np.random.seed(2019)   # 固定种子\n",
        "np.random.shuffle(idxes)\n",
        "print(idxes.shape, idxes[:10])\n",
        "\n",
        "\n",
        "# 8:1:1 划分训练集、验证集、测试集\n",
        "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:480]], input_ids[idxes[480:540]], input_ids[idxes[540:]]\n",
        "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:480]], input_masks[idxes[480:540]], input_masks[idxes[540:]] \n",
        "input_types_train, input_types_valid, input_types_test = input_types[idxes[:480]], input_types[idxes[480:540]], input_types[idxes[540:]]\n",
        "\n",
        "y_train, y_valid, y_test = labels[idxes[:480]], labels[idxes[480:540]], labels[idxes[540:]]\n",
        "\n",
        "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
        "      input_ids_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mijdSBjzH4",
        "outputId": "8068f7b0-fbdf-467c-dab4-624c2891bb5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600,) [520 266 127 527 260 351 515 391 478  41]\n",
            "(480, 20) (480,) (60, 20) (60,) (60, 20) (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2  # 如果会出现OOM问题，减小它\n",
        "# 训练集\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
        "                           torch.LongTensor(input_masks_train), \n",
        "                           torch.LongTensor(input_types_train), \n",
        "                           torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)  \n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 验证集\n",
        "valid_data = TensorDataset(torch.LongTensor(input_ids_valid), \n",
        "                          torch.LongTensor(input_masks_valid),\n",
        "                          torch.LongTensor(input_types_valid), \n",
        "                          torch.LongTensor(y_valid))\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 测试集（是没有标签的）\n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
        "                          torch.LongTensor(input_masks_test),\n",
        "                          torch.LongTensor(input_types_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_WgcrPVtk_qh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义model\n",
        "class Bert_Model(nn.Module):\n",
        "    def __init__(self, bert_path, classes=3):\n",
        "        super(Bert_Model, self).__init__()\n",
        "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
        "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
        "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
        "        logit = self.fc(out_pool)   #  [bs, classes]\n",
        "        return logit"
      ],
      "metadata": {
        "id": "0MsQTgjllJ6Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameter_number(model):\n",
        "    #  打印模型参数量\n",
        "    total_num = sum(p.numel() for p in model.parameters())\n",
        "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "model = Bert_Model(bert_path).to(DEVICE)\n",
        "print(get_parameter_number(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REZf4QmWlXsD",
        "outputId": "a88dc536-b1ca-4411-b556-6432db71db7e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/rb1/ were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 38479107, Trainable parameters: 38479107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(train_loader))"
      ],
      "metadata": {
        "id": "DqR29ttklpaL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估模型性能，在验证集上\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_true, val_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
        "    \n",
        "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
        "\n",
        "\n",
        "# 测试集没有标签，需要预测提交\n",
        "def predict(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "    return val_pred\n",
        "\n",
        "\n",
        "def train_and_eval(model, train_loader, valid_loader, \n",
        "                   optimizer, scheduler, device, epoch):\n",
        "    best_acc = 0.0\n",
        "    patience = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for i in range(epoch):\n",
        "        \"\"\"训练模型\"\"\"\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
        "        train_loss_sum = 0.0\n",
        "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
        "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
        "            y_pred = model(ids, att, tpe)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()   # 学习率变化\n",
        "            \n",
        "            train_loss_sum += loss.item()\n",
        "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
        "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
        "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
        "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
        "\n",
        "        \"\"\"验证模型\"\"\"\n",
        "        model.eval()\n",
        "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
        "        ## 保存最优模型\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
        "        \n",
        "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
        "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
      ],
      "metadata": {
        "id": "kRPhSVzUmT2Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPOvI8wmX3U",
        "outputId": "881fe92e-89fc-424d-d999-be4aefea3bbf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training epoch 1 *****\n",
            "Epoch 0001 | Step 0048/0240 | Loss 1.1503 | Time 1.2410\n",
            "Epoch 0001 | Step 0096/0240 | Loss 1.1049 | Time 2.4432\n",
            "Epoch 0001 | Step 0144/0240 | Loss 1.0742 | Time 3.6593\n",
            "Epoch 0001 | Step 0192/0240 | Loss 1.0359 | Time 4.9643\n",
            "Epoch 0001 | Step 0240/0240 | Loss 1.0224 | Time 6.0790\n",
            "current acc is 0.5500, best acc is 0.5500\n",
            "time costed = 6.50803s \n",
            "\n",
            "***** Running training epoch 2 *****\n",
            "Epoch 0002 | Step 0048/0240 | Loss 0.8999 | Time 1.0703\n",
            "Epoch 0002 | Step 0096/0240 | Loss 0.8838 | Time 2.4155\n",
            "Epoch 0002 | Step 0144/0240 | Loss 0.8221 | Time 3.4641\n",
            "Epoch 0002 | Step 0192/0240 | Loss 0.7994 | Time 4.5332\n",
            "Epoch 0002 | Step 0240/0240 | Loss 0.7835 | Time 5.5922\n",
            "current acc is 0.7333, best acc is 0.7333\n",
            "time costed = 6.00712s \n",
            "\n",
            "***** Running training epoch 3 *****\n",
            "Epoch 0003 | Step 0048/0240 | Loss 0.5610 | Time 1.0644\n",
            "Epoch 0003 | Step 0096/0240 | Loss 0.5004 | Time 2.1226\n",
            "Epoch 0003 | Step 0144/0240 | Loss 0.4526 | Time 3.2485\n",
            "Epoch 0003 | Step 0192/0240 | Loss 0.4321 | Time 4.4712\n",
            "Epoch 0003 | Step 0240/0240 | Loss 0.4588 | Time 5.6783\n",
            "current acc is 0.8000, best acc is 0.8000\n",
            "time costed = 6.32308s \n",
            "\n",
            "***** Running training epoch 4 *****\n",
            "Epoch 0004 | Step 0048/0240 | Loss 0.2591 | Time 1.3152\n",
            "Epoch 0004 | Step 0096/0240 | Loss 0.2527 | Time 2.4394\n",
            "Epoch 0004 | Step 0144/0240 | Loss 0.2231 | Time 3.4888\n",
            "Epoch 0004 | Step 0192/0240 | Loss 0.2056 | Time 4.5400\n",
            "Epoch 0004 | Step 0240/0240 | Loss 0.2040 | Time 5.5936\n",
            "current acc is 0.8333, best acc is 0.8333\n",
            "time costed = 5.99463s \n",
            "\n",
            "***** Running training epoch 5 *****\n",
            "Epoch 0005 | Step 0048/0240 | Loss 0.1582 | Time 1.0743\n",
            "Epoch 0005 | Step 0096/0240 | Loss 0.1589 | Time 2.1560\n",
            "Epoch 0005 | Step 0144/0240 | Loss 0.1330 | Time 3.1996\n",
            "Epoch 0005 | Step 0192/0240 | Loss 0.1257 | Time 4.2484\n",
            "Epoch 0005 | Step 0240/0240 | Loss 0.1247 | Time 5.2944\n",
            "current acc is 0.8833, best acc is 0.8833\n",
            "time costed = 5.74477s \n",
            "\n",
            "***** Running training epoch 6 *****\n",
            "Epoch 0006 | Step 0048/0240 | Loss 0.0510 | Time 1.2295\n",
            "Epoch 0006 | Step 0096/0240 | Loss 0.0748 | Time 2.4534\n",
            "Epoch 0006 | Step 0144/0240 | Loss 0.0755 | Time 3.6740\n",
            "Epoch 0006 | Step 0192/0240 | Loss 0.0782 | Time 4.9585\n",
            "Epoch 0006 | Step 0240/0240 | Loss 0.0789 | Time 6.0704\n",
            "current acc is 0.8833, best acc is 0.8833\n",
            "time costed = 6.19507s \n",
            "\n",
            "***** Running training epoch 7 *****\n",
            "Epoch 0007 | Step 0048/0240 | Loss 0.0291 | Time 1.0631\n",
            "Epoch 0007 | Step 0096/0240 | Loss 0.0319 | Time 2.1196\n",
            "Epoch 0007 | Step 0144/0240 | Loss 0.0378 | Time 3.1667\n",
            "Epoch 0007 | Step 0192/0240 | Loss 0.0417 | Time 4.2205\n",
            "Epoch 0007 | Step 0240/0240 | Loss 0.0387 | Time 5.2594\n",
            "current acc is 0.8667, best acc is 0.8833\n",
            "time costed = 5.39028s \n",
            "\n",
            "***** Running training epoch 8 *****\n",
            "Epoch 0008 | Step 0048/0240 | Loss 0.0263 | Time 1.0477\n",
            "Epoch 0008 | Step 0096/0240 | Loss 0.0316 | Time 2.0880\n",
            "Epoch 0008 | Step 0144/0240 | Loss 0.0390 | Time 3.1327\n",
            "Epoch 0008 | Step 0192/0240 | Loss 0.0354 | Time 4.2690\n",
            "Epoch 0008 | Step 0240/0240 | Loss 0.0338 | Time 5.4943\n",
            "current acc is 0.8833, best acc is 0.8833\n",
            "time costed = 5.68221s \n",
            "\n",
            "***** Running training epoch 9 *****\n",
            "Epoch 0009 | Step 0048/0240 | Loss 0.0145 | Time 1.1967\n",
            "Epoch 0009 | Step 0096/0240 | Loss 0.0170 | Time 2.4681\n",
            "Epoch 0009 | Step 0144/0240 | Loss 0.0164 | Time 3.6633\n",
            "Epoch 0009 | Step 0192/0240 | Loss 0.0166 | Time 4.7184\n",
            "Epoch 0009 | Step 0240/0240 | Loss 0.0213 | Time 5.7789\n",
            "current acc is 0.8833, best acc is 0.8833\n",
            "time costed = 5.91238s \n",
            "\n",
            "***** Running training epoch 10 *****\n",
            "Epoch 0010 | Step 0048/0240 | Loss 0.0171 | Time 1.0609\n",
            "Epoch 0010 | Step 0096/0240 | Loss 0.0165 | Time 2.1143\n",
            "Epoch 0010 | Step 0144/0240 | Loss 0.0206 | Time 3.1773\n",
            "Epoch 0010 | Step 0192/0240 | Loss 0.0213 | Time 4.2291\n",
            "Epoch 0010 | Step 0240/0240 | Loss 0.0204 | Time 5.2970\n",
            "current acc is 0.8833, best acc is 0.8833\n",
            "time costed = 5.434s \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载最优权重对测试集测试\n",
        "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
        "pred_test = predict(model, test_loader, DEVICE)\n",
        "# print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(\"\\n Test Accuracy = {:.3f} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(classification_report(y_test, pred_test, digits=4))"
      ],
      "metadata": {
        "id": "7h6u65AkmiDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330ab5cd-0ec4-4dd8-d583-e8719f24e108"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30it [00:00, 165.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy = 0.833 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9286    0.8667    0.8966        30\n",
            "           1     0.8095    0.8095    0.8095        21\n",
            "           2     0.6364    0.7778    0.7000         9\n",
            "\n",
            "    accuracy                         0.8333        60\n",
            "   macro avg     0.7915    0.8180    0.8020        60\n",
            "weighted avg     0.8431    0.8333    0.8366        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = 'BBU与AUU告警'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "d2b-G_38KTBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f4c6f3-31dd-44b5-965e-548d8e711b81"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bbu', '与', 'au', '##u', '告', '警']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_added_toks = tokenizer.add_tokens([\"BBU\",\"AUU\"])\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQdXg6HUgTDY",
        "outputId": "5af60bca-16fd-4327-c590-2a907c40c5ea"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bbu', '与', 'auu', '告', '警']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tokenizer(text)\n",
        "a"
      ],
      "metadata": {
        "id": "RrpdXltXqK24",
        "outputId": "f61450f4-1ffb-4053-c919-969322fbce6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 21128, 680, 21129, 1440, 6356, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovN5idn0qfvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}