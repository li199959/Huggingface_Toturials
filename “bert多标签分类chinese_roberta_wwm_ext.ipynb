{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/li199959/one/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB1%E2%80%9D.ipynb",
      "authorship_tag": "ABX9TyM/34afEypNpSfnf/qrwpQD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/Huggingface_Toturials/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BBchinese_roberta_wwm_ext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhpxN3Gwdhgx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json, time \n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "bert_path = \"/content/drive/MyDrive/chinese_roberta_wwm_ext_pytorch/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
      ],
      "metadata": {
        "id": "ky5y_ZFBd55Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f76eae1-4c9c-4b9c-c101-00721af02a2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_masks, input_types,  = [], [], []  # input char ids, segment type ids,  attention mask\n",
        "labels = []         # 标签\n",
        "maxlen = 20      # 取30即可覆盖99%\n",
        " \n",
        "with open(\"news_title_dataset.csv\", encoding='utf-8') as f:\n",
        "    for i, line in tqdm(enumerate(f)): \n",
        "        title, y = line.strip().split('\\t')\n",
        "\n",
        "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
        "        # 根据参数会短则补齐，长则切断\n",
        "        encode_dict = tokenizer.encode_plus(text=title, max_length=maxlen, \n",
        "                                            padding='max_length', truncation=True)\n",
        "        \n",
        "        input_ids.append(encode_dict['input_ids'])\n",
        "        input_types.append(encode_dict['token_type_ids'])\n",
        "        input_masks.append(encode_dict['attention_mask'])\n",
        "\n",
        "        labels.append(int(y))\n",
        "\n",
        "input_ids, input_types, input_masks = np.array(input_ids), np.array(input_types), np.array(input_masks)\n",
        "labels = np.array(labels)\n",
        "print(input_ids.shape, input_types.shape, input_masks.shape, labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYodgs1eLYC",
        "outputId": "23106f12-ccbe-4232-c9f0-e36591b686c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [00:00, 2842.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 20) (600, 20) (600, 20) (600,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机打乱索引\n",
        "idxes = np.arange(input_ids.shape[0])\n",
        "np.random.seed(2019)   # 固定种子\n",
        "np.random.shuffle(idxes)\n",
        "print(idxes.shape, idxes[:10])\n",
        "\n",
        "\n",
        "# 8:1:1 划分训练集、验证集、测试集\n",
        "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:480]], input_ids[idxes[480:540]], input_ids[idxes[540:]]\n",
        "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:480]], input_masks[idxes[480:540]], input_masks[idxes[540:]] \n",
        "input_types_train, input_types_valid, input_types_test = input_types[idxes[:480]], input_types[idxes[480:540]], input_types[idxes[540:]]\n",
        "\n",
        "y_train, y_valid, y_test = labels[idxes[:480]], labels[idxes[480:540]], labels[idxes[540:]]\n",
        "\n",
        "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
        "      input_ids_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mijdSBjzH4",
        "outputId": "eb5d7301-8ddd-4d8e-b042-2bc421477d54"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600,) [520 266 127 527 260 351 515 391 478  41]\n",
            "(480, 20) (480,) (60, 20) (60,) (60, 20) (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8  # 如果会出现OOM问题，减小它\n",
        "# 训练集\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
        "                           torch.LongTensor(input_masks_train), \n",
        "                           torch.LongTensor(input_types_train), \n",
        "                           torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)  \n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 验证集\n",
        "valid_data = TensorDataset(torch.LongTensor(input_ids_valid), \n",
        "                          torch.LongTensor(input_masks_valid),\n",
        "                          torch.LongTensor(input_types_valid), \n",
        "                          torch.LongTensor(y_valid))\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 测试集（是没有标签的）\n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
        "                          torch.LongTensor(input_masks_test),\n",
        "                          torch.LongTensor(input_types_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_WgcrPVtk_qh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义model\n",
        "class Bert_Model(nn.Module):\n",
        "    def __init__(self, bert_path, classes=3):\n",
        "        super(Bert_Model, self).__init__()\n",
        "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
        "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
        "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
        "        logit = self.fc(out_pool)   #  [bs, classes]\n",
        "        return logit"
      ],
      "metadata": {
        "id": "0MsQTgjllJ6Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameter_number(model):\n",
        "    #  打印模型参数量\n",
        "    total_num = sum(p.numel() for p in model.parameters())\n",
        "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "model = Bert_Model(bert_path).to(DEVICE)\n",
        "print(get_parameter_number(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REZf4QmWlXsD",
        "outputId": "c4e5120b-a1a1-496e-926d-0c80297002c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/chinese_roberta_wwm_ext_pytorch/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 102269955, Trainable parameters: 102269955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(train_loader))"
      ],
      "metadata": {
        "id": "DqR29ttklpaL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7rDAb3FY7Bbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07ffde7-2da1-4693-d6be-72a11c32bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估模型性能，在验证集上\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_true, val_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
        "    \n",
        "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
        "\n",
        "\n",
        "# 测试集没有标签，需要预测提交\n",
        "def predict(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "    return val_pred\n",
        "\n",
        "\n",
        "def train_and_eval(model, train_loader, valid_loader, \n",
        "                   optimizer, scheduler, device, epoch):\n",
        "    best_acc = 0.0\n",
        "    patience = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for i in range(epoch):\n",
        "        \"\"\"训练模型\"\"\"\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
        "        train_loss_sum = 0.0\n",
        "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
        "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
        "            y_pred = model(ids, att, tpe)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()   # 学习率变化\n",
        "            \n",
        "            train_loss_sum += loss.item()\n",
        "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
        "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
        "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
        "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
        "\n",
        "        \"\"\"验证模型\"\"\"\n",
        "        model.eval()\n",
        "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
        "        ## 保存最优模型\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
        "        \n",
        "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
        "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
      ],
      "metadata": {
        "id": "kRPhSVzUmT2Z"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPOvI8wmX3U",
        "outputId": "227527e9-2a5d-49c9-b035-0c948cee73c0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training epoch 1 *****\n",
            "Epoch 0001 | Step 0012/0060 | Loss 1.1014 | Time 0.8955\n",
            "Epoch 0001 | Step 0024/0060 | Loss 1.0717 | Time 1.7249\n",
            "Epoch 0001 | Step 0036/0060 | Loss 1.0317 | Time 2.5565\n",
            "Epoch 0001 | Step 0048/0060 | Loss 1.0280 | Time 3.3825\n",
            "Epoch 0001 | Step 0060/0060 | Loss 1.0020 | Time 4.2084\n",
            "current acc is 0.4167, best acc is 0.4167\n",
            "time costed = 5.40813s \n",
            "\n",
            "***** Running training epoch 2 *****\n",
            "Epoch 0002 | Step 0012/0060 | Loss 0.8717 | Time 0.8402\n",
            "Epoch 0002 | Step 0024/0060 | Loss 0.7956 | Time 1.6633\n",
            "Epoch 0002 | Step 0036/0060 | Loss 0.7753 | Time 2.4925\n",
            "Epoch 0002 | Step 0048/0060 | Loss 0.7497 | Time 3.4094\n",
            "Epoch 0002 | Step 0060/0060 | Loss 0.7119 | Time 4.3605\n",
            "current acc is 0.7167, best acc is 0.7167\n",
            "time costed = 5.76108s \n",
            "\n",
            "***** Running training epoch 3 *****\n",
            "Epoch 0003 | Step 0012/0060 | Loss 0.4329 | Time 0.9996\n",
            "Epoch 0003 | Step 0024/0060 | Loss 0.3929 | Time 2.0190\n",
            "Epoch 0003 | Step 0036/0060 | Loss 0.4041 | Time 2.9712\n",
            "Epoch 0003 | Step 0048/0060 | Loss 0.4100 | Time 3.8092\n",
            "Epoch 0003 | Step 0060/0060 | Loss 0.4032 | Time 4.6438\n",
            "current acc is 0.8333, best acc is 0.8333\n",
            "time costed = 5.85044s \n",
            "\n",
            "***** Running training epoch 4 *****\n",
            "Epoch 0004 | Step 0012/0060 | Loss 0.2040 | Time 0.8436\n",
            "Epoch 0004 | Step 0024/0060 | Loss 0.1694 | Time 1.6852\n",
            "Epoch 0004 | Step 0036/0060 | Loss 0.1953 | Time 2.5225\n",
            "Epoch 0004 | Step 0048/0060 | Loss 0.1990 | Time 3.3536\n",
            "Epoch 0004 | Step 0060/0060 | Loss 0.2118 | Time 4.1922\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 5.39176s \n",
            "\n",
            "***** Running training epoch 5 *****\n",
            "Epoch 0005 | Step 0012/0060 | Loss 0.1126 | Time 0.8421\n",
            "Epoch 0005 | Step 0024/0060 | Loss 0.1109 | Time 1.7144\n",
            "Epoch 0005 | Step 0036/0060 | Loss 0.1106 | Time 2.6665\n",
            "Epoch 0005 | Step 0048/0060 | Loss 0.1041 | Time 3.5879\n",
            "Epoch 0005 | Step 0060/0060 | Loss 0.1022 | Time 4.5406\n",
            "current acc is 0.8833, best acc is 0.9000\n",
            "time costed = 4.6912s \n",
            "\n",
            "***** Running training epoch 6 *****\n",
            "Epoch 0006 | Step 0012/0060 | Loss 0.0648 | Time 0.9915\n",
            "Epoch 0006 | Step 0024/0060 | Loss 0.0576 | Time 2.0257\n",
            "Epoch 0006 | Step 0036/0060 | Loss 0.0519 | Time 2.8883\n",
            "Epoch 0006 | Step 0048/0060 | Loss 0.0566 | Time 3.7337\n",
            "Epoch 0006 | Step 0060/0060 | Loss 0.0533 | Time 4.5853\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 4.71337s \n",
            "\n",
            "***** Running training epoch 7 *****\n",
            "Epoch 0007 | Step 0012/0060 | Loss 0.0270 | Time 0.8449\n",
            "Epoch 0007 | Step 0024/0060 | Loss 0.0262 | Time 1.6806\n",
            "Epoch 0007 | Step 0036/0060 | Loss 0.0369 | Time 2.5178\n",
            "Epoch 0007 | Step 0048/0060 | Loss 0.0338 | Time 3.3643\n",
            "Epoch 0007 | Step 0060/0060 | Loss 0.0323 | Time 4.2165\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 4.33381s \n",
            "\n",
            "***** Running training epoch 8 *****\n",
            "Epoch 0008 | Step 0012/0060 | Loss 0.0166 | Time 0.8459\n",
            "Epoch 0008 | Step 0024/0060 | Loss 0.0188 | Time 1.6787\n",
            "Epoch 0008 | Step 0036/0060 | Loss 0.0181 | Time 2.5203\n",
            "Epoch 0008 | Step 0048/0060 | Loss 0.0196 | Time 3.4128\n",
            "Epoch 0008 | Step 0060/0060 | Loss 0.0191 | Time 4.3535\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 4.51261s \n",
            "\n",
            "***** Running training epoch 9 *****\n",
            "Epoch 0009 | Step 0012/0060 | Loss 0.0174 | Time 0.9481\n",
            "Epoch 0009 | Step 0024/0060 | Loss 0.0162 | Time 1.8890\n",
            "Epoch 0009 | Step 0036/0060 | Loss 0.0177 | Time 2.8947\n",
            "Epoch 0009 | Step 0048/0060 | Loss 0.0172 | Time 3.9240\n",
            "Epoch 0009 | Step 0060/0060 | Loss 0.0163 | Time 4.7742\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 4.88998s \n",
            "\n",
            "***** Running training epoch 10 *****\n",
            "Epoch 0010 | Step 0012/0060 | Loss 0.0186 | Time 0.8427\n",
            "Epoch 0010 | Step 0024/0060 | Loss 0.0181 | Time 1.6790\n",
            "Epoch 0010 | Step 0036/0060 | Loss 0.0191 | Time 2.5100\n",
            "Epoch 0010 | Step 0048/0060 | Loss 0.0177 | Time 3.3444\n",
            "Epoch 0010 | Step 0060/0060 | Loss 0.0171 | Time 4.1827\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 4.29813s \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载最优权重对测试集测试\n",
        "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
        "pred_test = predict(model, test_loader, DEVICE)\n",
        "# print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(\"\\n Test Accuracy = {:.3f} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(classification_report(y_test, pred_test, digits=4))"
      ],
      "metadata": {
        "id": "7h6u65AkmiDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf9d48f-375d-4882-d435-4e2b79580d52"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8it [00:00, 44.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy = 0.850 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8667    0.8667    0.8667        30\n",
            "           1     0.8889    0.7619    0.8205        21\n",
            "           2     0.7500    1.0000    0.8571         9\n",
            "\n",
            "    accuracy                         0.8500        60\n",
            "   macro avg     0.8352    0.8762    0.8481        60\n",
            "weighted avg     0.8569    0.8500    0.8491        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=tokenizer('告警分类')\n",
        "a"
      ],
      "metadata": {
        "id": "d2b-G_38KTBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd19dba4-018d-46fa-820f-2032300698cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1440, 6356, 1146, 5102, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQdXg6HUgTDY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}