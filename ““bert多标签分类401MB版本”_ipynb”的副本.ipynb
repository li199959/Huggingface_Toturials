{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/li199959/one/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB1%E2%80%9D.ipynb",
      "authorship_tag": "ABX9TyNI44SDB6o1da/41DnL3F5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/Huggingface_Toturials/blob/main/%E2%80%9C%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB401MB%E7%89%88%E6%9C%AC%E2%80%9D_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhpxN3Gwdhgx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json, time \n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "bert_path = \"/content/drive/MyDrive/bert_model/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
      ],
      "metadata": {
        "id": "ky5y_ZFBd55Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_masks, input_types,  = [], [], []  # input char ids, segment type ids,  attention mask\n",
        "labels = []         # 标签\n",
        "maxlen = 20      # 取30即可覆盖99%\n",
        " \n",
        "with open(\"news_title_dataset.csv\", encoding='utf-8') as f:\n",
        "    for i, line in tqdm(enumerate(f)): \n",
        "        title, y = line.strip().split('\\t')\n",
        "\n",
        "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
        "        # 根据参数会短则补齐，长则切断\n",
        "        encode_dict = tokenizer.encode_plus(text=title, max_length=maxlen, \n",
        "                                            padding='max_length', truncation=True)\n",
        "        \n",
        "        input_ids.append(encode_dict['input_ids'])\n",
        "        input_types.append(encode_dict['token_type_ids'])\n",
        "        input_masks.append(encode_dict['attention_mask'])\n",
        "\n",
        "        labels.append(int(y))\n",
        "\n",
        "input_ids, input_types, input_masks = np.array(input_ids), np.array(input_types), np.array(input_masks)\n",
        "labels = np.array(labels)\n",
        "print(input_ids.shape, input_types.shape, input_masks.shape, labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYodgs1eLYC",
        "outputId": "3bcb704a-5a07-4cb5-8f5f-40960539ad66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [00:00, 1320.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 20) (600, 20) (600, 20) (600,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机打乱索引\n",
        "idxes = np.arange(input_ids.shape[0])\n",
        "np.random.seed(2019)   # 固定种子\n",
        "np.random.shuffle(idxes)\n",
        "print(idxes.shape, idxes[:10])\n",
        "\n",
        "\n",
        "# 8:1:1 划分训练集、验证集、测试集\n",
        "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:480]], input_ids[idxes[480:540]], input_ids[idxes[540:]]\n",
        "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:480]], input_masks[idxes[480:540]], input_masks[idxes[540:]] \n",
        "input_types_train, input_types_valid, input_types_test = input_types[idxes[:480]], input_types[idxes[480:540]], input_types[idxes[540:]]\n",
        "\n",
        "y_train, y_valid, y_test = labels[idxes[:480]], labels[idxes[480:540]], labels[idxes[540:]]\n",
        "\n",
        "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
        "      input_ids_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mijdSBjzH4",
        "outputId": "9bd0e050-4a4e-4d0c-ac87-5137b2632951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600,) [520 266 127 527 260 351 515 391 478  41]\n",
            "(480, 20) (480,) (60, 20) (60,) (60, 20) (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4  # 如果会出现OOM问题，减小它\n",
        "# 训练集\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
        "                           torch.LongTensor(input_masks_train), \n",
        "                           torch.LongTensor(input_types_train), \n",
        "                           torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)  \n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 验证集\n",
        "valid_data = TensorDataset(torch.LongTensor(input_ids_valid), \n",
        "                          torch.LongTensor(input_masks_valid),\n",
        "                          torch.LongTensor(input_types_valid), \n",
        "                          torch.LongTensor(y_valid))\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 测试集（是没有标签的）\n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
        "                          torch.LongTensor(input_masks_test),\n",
        "                          torch.LongTensor(input_types_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_WgcrPVtk_qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义model\n",
        "class Bert_Model(nn.Module):\n",
        "    def __init__(self, bert_path, classes=3):\n",
        "        super(Bert_Model, self).__init__()\n",
        "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
        "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
        "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
        "        logit = self.fc(out_pool)   #  [bs, classes]\n",
        "        return logit"
      ],
      "metadata": {
        "id": "0MsQTgjllJ6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameter_number(model):\n",
        "    #  打印模型参数量\n",
        "    total_num = sum(p.numel() for p in model.parameters())\n",
        "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "model = Bert_Model(bert_path).to(DEVICE)\n",
        "print(get_parameter_number(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REZf4QmWlXsD",
        "outputId": "c60ced0b-919a-4615-f777-56dbfa95f4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/bert_model/ were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 102269955, Trainable parameters: 102269955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(train_loader))"
      ],
      "metadata": {
        "id": "DqR29ttklpaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7rDAb3FY7Bbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07ffde7-2da1-4693-d6be-72a11c32bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估模型性能，在验证集上\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_true, val_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
        "    \n",
        "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
        "\n",
        "\n",
        "# 测试集没有标签，需要预测提交\n",
        "def predict(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "    return val_pred\n",
        "\n",
        "\n",
        "def train_and_eval(model, train_loader, valid_loader, \n",
        "                   optimizer, scheduler, device, epoch):\n",
        "    best_acc = 0.0\n",
        "    patience = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for i in range(epoch):\n",
        "        \"\"\"训练模型\"\"\"\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
        "        train_loss_sum = 0.0\n",
        "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
        "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
        "            y_pred = model(ids, att, tpe)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()   # 学习率变化\n",
        "            \n",
        "            train_loss_sum += loss.item()\n",
        "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
        "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
        "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
        "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
        "\n",
        "        \"\"\"验证模型\"\"\"\n",
        "        model.eval()\n",
        "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
        "        ## 保存最优模型\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
        "        \n",
        "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
        "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
      ],
      "metadata": {
        "id": "kRPhSVzUmT2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPOvI8wmX3U",
        "outputId": "3df7c209-7335-44e8-eca0-56aa64d32a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training epoch 1 *****\n",
            "Epoch 0001 | Step 0024/0120 | Loss 1.2513 | Time 1.7666\n",
            "Epoch 0001 | Step 0048/0120 | Loss 1.1543 | Time 3.6394\n",
            "Epoch 0001 | Step 0072/0120 | Loss 1.1149 | Time 5.5671\n",
            "Epoch 0001 | Step 0096/0120 | Loss 1.0995 | Time 7.2607\n",
            "Epoch 0001 | Step 0120/0120 | Loss 1.0710 | Time 8.8145\n",
            "current acc is 0.4500, best acc is 0.4500\n",
            "time costed = 10.08578s \n",
            "\n",
            "***** Running training epoch 2 *****\n",
            "Epoch 0002 | Step 0024/0120 | Loss 0.9377 | Time 1.6124\n",
            "Epoch 0002 | Step 0048/0120 | Loss 0.8762 | Time 3.1637\n",
            "Epoch 0002 | Step 0072/0120 | Loss 0.8120 | Time 4.7074\n",
            "Epoch 0002 | Step 0096/0120 | Loss 0.8218 | Time 6.2714\n",
            "Epoch 0002 | Step 0120/0120 | Loss 0.8071 | Time 8.1317\n",
            "current acc is 0.7500, best acc is 0.7500\n",
            "time costed = 9.6567s \n",
            "\n",
            "***** Running training epoch 3 *****\n",
            "Epoch 0003 | Step 0024/0120 | Loss 0.6058 | Time 1.9672\n",
            "Epoch 0003 | Step 0048/0120 | Loss 0.5477 | Time 3.4974\n",
            "Epoch 0003 | Step 0072/0120 | Loss 0.5098 | Time 5.0758\n",
            "Epoch 0003 | Step 0096/0120 | Loss 0.4762 | Time 6.6191\n",
            "Epoch 0003 | Step 0120/0120 | Loss 0.4646 | Time 8.1595\n",
            "current acc is 0.8167, best acc is 0.8167\n",
            "time costed = 9.40765s \n",
            "\n",
            "***** Running training epoch 4 *****\n",
            "Epoch 0004 | Step 0024/0120 | Loss 0.2837 | Time 1.5846\n",
            "Epoch 0004 | Step 0048/0120 | Loss 0.3426 | Time 3.3041\n",
            "Epoch 0004 | Step 0072/0120 | Loss 0.3389 | Time 5.1741\n",
            "Epoch 0004 | Step 0096/0120 | Loss 0.3346 | Time 7.6149\n",
            "Epoch 0004 | Step 0120/0120 | Loss 0.3078 | Time 9.7896\n",
            "current acc is 0.8333, best acc is 0.8333\n",
            "time costed = 11.12415s \n",
            "\n",
            "***** Running training epoch 5 *****\n",
            "Epoch 0005 | Step 0024/0120 | Loss 0.1679 | Time 1.5768\n",
            "Epoch 0005 | Step 0048/0120 | Loss 0.1294 | Time 3.1312\n",
            "Epoch 0005 | Step 0072/0120 | Loss 0.1162 | Time 4.6774\n",
            "Epoch 0005 | Step 0096/0120 | Loss 0.1252 | Time 6.2358\n",
            "Epoch 0005 | Step 0120/0120 | Loss 0.1247 | Time 7.9861\n",
            "current acc is 0.8667, best acc is 0.8667\n",
            "time costed = 9.51953s \n",
            "\n",
            "***** Running training epoch 6 *****\n",
            "Epoch 0006 | Step 0024/0120 | Loss 0.0579 | Time 1.9836\n",
            "Epoch 0006 | Step 0048/0120 | Loss 0.0692 | Time 3.7026\n",
            "Epoch 0006 | Step 0072/0120 | Loss 0.0587 | Time 5.2805\n",
            "Epoch 0006 | Step 0096/0120 | Loss 0.0582 | Time 6.8399\n",
            "Epoch 0006 | Step 0120/0120 | Loss 0.0533 | Time 8.3910\n",
            "current acc is 0.9000, best acc is 0.9000\n",
            "time costed = 9.65499s \n",
            "\n",
            "***** Running training epoch 7 *****\n",
            "Epoch 0007 | Step 0024/0120 | Loss 0.0214 | Time 1.5743\n",
            "Epoch 0007 | Step 0048/0120 | Loss 0.0337 | Time 3.1785\n",
            "Epoch 0007 | Step 0072/0120 | Loss 0.0297 | Time 5.0135\n",
            "Epoch 0007 | Step 0096/0120 | Loss 0.0272 | Time 6.8850\n",
            "Epoch 0007 | Step 0120/0120 | Loss 0.0239 | Time 8.7437\n",
            "current acc is 0.8667, best acc is 0.9000\n",
            "time costed = 8.96747s \n",
            "\n",
            "***** Running training epoch 8 *****\n",
            "Epoch 0008 | Step 0024/0120 | Loss 0.0126 | Time 1.5848\n",
            "Epoch 0008 | Step 0048/0120 | Loss 0.0122 | Time 3.5229\n",
            "Epoch 0008 | Step 0072/0120 | Loss 0.0148 | Time 5.0740\n",
            "Epoch 0008 | Step 0096/0120 | Loss 0.0153 | Time 6.6059\n",
            "Epoch 0008 | Step 0120/0120 | Loss 0.0138 | Time 8.1950\n",
            "current acc is 0.8833, best acc is 0.9000\n",
            "time costed = 8.40019s \n",
            "\n",
            "***** Running training epoch 9 *****\n",
            "Epoch 0009 | Step 0024/0120 | Loss 0.0071 | Time 1.6947\n",
            "Epoch 0009 | Step 0048/0120 | Loss 0.0080 | Time 3.5607\n",
            "Epoch 0009 | Step 0072/0120 | Loss 0.0095 | Time 5.5231\n",
            "Epoch 0009 | Step 0096/0120 | Loss 0.0102 | Time 7.2305\n",
            "Epoch 0009 | Step 0120/0120 | Loss 0.0108 | Time 8.7982\n",
            "current acc is 0.8833, best acc is 0.9000\n",
            "time costed = 9.02775s \n",
            "\n",
            "***** Running training epoch 10 *****\n",
            "Epoch 0010 | Step 0024/0120 | Loss 0.0113 | Time 1.5253\n",
            "Epoch 0010 | Step 0048/0120 | Loss 0.0096 | Time 3.0742\n",
            "Epoch 0010 | Step 0072/0120 | Loss 0.0104 | Time 4.6441\n",
            "Epoch 0010 | Step 0096/0120 | Loss 0.0111 | Time 6.1978\n",
            "Epoch 0010 | Step 0120/0120 | Loss 0.0112 | Time 7.8731\n",
            "current acc is 0.8833, best acc is 0.9000\n",
            "time costed = 8.16126s \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载最优权重对测试集测试\n",
        "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
        "pred_test = predict(model, test_loader, DEVICE)\n",
        "# print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(\"\\n Test Accuracy = {:.3f} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(classification_report(y_test, pred_test, digits=4))"
      ],
      "metadata": {
        "id": "7h6u65AkmiDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01128fce-ded8-4b0d-9803-9343b4598206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15it [00:00, 48.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy = 0.833 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8710    0.9000    0.8852        30\n",
            "           1     0.8500    0.8095    0.8293        21\n",
            "           2     0.6667    0.6667    0.6667         9\n",
            "\n",
            "    accuracy                         0.8333        60\n",
            "   macro avg     0.7959    0.7921    0.7937        60\n",
            "weighted avg     0.8330    0.8333    0.8329        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2b-G_38KTBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}